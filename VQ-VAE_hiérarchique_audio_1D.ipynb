{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b913c60c",
   "metadata": {},
   "source": [
    "Notebook — VQ-VAE hiérarchique audio 1D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4491393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83573e82",
   "metadata": {},
   "source": [
    "Hyper-paramètres (article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7cd41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "DURATION = 4  # secondes pour le test\n",
    "NUM_SAMPLES = SAMPLE_RATE * DURATION\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LR = 2e-4\n",
    "EPOCHS = 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4cea56",
   "metadata": {},
   "source": [
    "# Architecture VQ-VAE Hiérarchique - Audio\n",
    "\n",
    "Signal audio brut (4s, 16kHz, 64 000 échantillons)  \n",
    "      │  \n",
    "      ▼  \n",
    "[Encodeur Audio 1D - 5 couches conv, stride 2]  \n",
    "      │  \n",
    "      │  \n",
    "      ├─ Après 3 convolutions → **h_low (LOW)**  \n",
    "      │      ↓ downsampling factor ≈ 8  \n",
    "      │      ↓ shape : (BATCH_SIZE, hidden=128, T_low=8000)  \n",
    "      │      │  \n",
    "      │      ▼  \n",
    "      │   [VQ Low]  \n",
    "      │      ↓ quantification  \n",
    "      │      ↓ output : **z_low** (BBATCH_SIZE, 128, 8000)  \n",
    "      │  \n",
    "      └─ Après 5 convolutions → **h_up (UP)**  \n",
    "             ↓ downsampling factor ≈ 32  \n",
    "             ↓ shape : (BATCH_SIZE, hidden=128, T_up=2000)  \n",
    "             │  \n",
    "             ▼  \n",
    "         [VQ Up]  \n",
    "             ↓ quantification  \n",
    "             ↓ output : **z_up** (BATCH_SIZE, 128, 2000)  \n",
    "             │  \n",
    "             ▼  \n",
    "[Décodage Audio 1D - 5 couches conv transpose]  \n",
    "             │  \n",
    "             ▼  \n",
    "Signal audio reconstruit (4s, 16kHz, 64 000 échantillons)  \n",
    "\n",
    "---\n",
    "\n",
    "### Explications :\n",
    "- **Hiérarchie multi-niveaux** :  \n",
    "  - LOW capture la **structure globale / rythme**  \n",
    "  - UP capture le **timbre et les détails fins**  \n",
    "- Chaque niveau possède son propre **codebook VQ** (2048 codes, dim 128)  \n",
    "- Les vecteurs latents sont **discrets** grâce à la quantification  \n",
    "- La reconstruction se fait uniquement à partir du latent **UP** dans ce modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64643ced",
   "metadata": {},
   "source": [
    "Dataset audio (signal brut 1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfbb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        self.file_list = file_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav, sr = torchaudio.load(self.file_list[idx])\n",
    "        wav = wav.mean(dim=0)  # mono\n",
    "\n",
    "        if sr != SAMPLE_RATE:\n",
    "            wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)\n",
    "\n",
    "        wav = wav[:NUM_SAMPLES]\n",
    "        if wav.shape[0] < NUM_SAMPLES:\n",
    "            wav = F.pad(wav, (0, NUM_SAMPLES - wav.shape[0]))\n",
    "\n",
    "        return wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a90bd",
   "metadata": {},
   "source": [
    "Encodeur 1D hiérarchique (5 convs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9440cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, hidden=128):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(1, hidden, 4, stride=2, padding=1),\n",
    "            nn.Conv1d(hidden, hidden, 4, stride=2, padding=1),\n",
    "            nn.Conv1d(hidden, hidden, 4, stride=2, padding=1),  # LOW\n",
    "            nn.Conv1d(hidden, hidden, 4, stride=2, padding=1),\n",
    "            nn.Conv1d(hidden, hidden, 4, stride=2, padding=1),  # UP\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x.unsqueeze(1)  # (B, 1, T)\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            h = F.relu(conv(h))\n",
    "            if i == 2:\n",
    "                h_low = h\n",
    "        h_up = h\n",
    "        return h_low, h_up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852380a",
   "metadata": {},
   "source": [
    "Vector Quantizer (VQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e4cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_codes=2048, dim=128, beta=0.25):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_codes, dim)\n",
    "        self.embedding.weight.data.uniform_(-1/num_codes, 1/num_codes)\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, z):\n",
    "        z_perm = z.permute(0, 2, 1).contiguous()\n",
    "        z_flat = z_perm.view(-1, z_perm.size(-1))\n",
    "\n",
    "        dist = (\n",
    "            z_flat.pow(2).sum(1, keepdim=True)\n",
    "            - 2 * z_flat @ self.embedding.weight.t()\n",
    "            + self.embedding.weight.pow(2).sum(1)\n",
    "        )\n",
    "\n",
    "        indices = dist.argmin(1)\n",
    "        z_q = self.embedding(indices).view(z_perm.shape)\n",
    "        z_q = z_q.permute(0, 2, 1)\n",
    "\n",
    "        loss = (\n",
    "            (z_q.detach() - z).pow(2).mean()\n",
    "            + self.beta * (z_q - z.detach()).pow(2).mean()\n",
    "        )\n",
    "\n",
    "        z_q = z + (z_q - z).detach()\n",
    "        return z_q, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34349b12",
   "metadata": {},
   "source": [
    "Décodeur 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9198f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDecoder(nn.Module):\n",
    "    def __init__(self, hidden=128):\n",
    "        super().__init__()\n",
    "        self.deconvs = nn.ModuleList([\n",
    "            nn.ConvTranspose1d(hidden, hidden, 4, 2, 1),\n",
    "            nn.ConvTranspose1d(hidden, hidden, 4, 2, 1),\n",
    "            nn.ConvTranspose1d(hidden, hidden, 4, 2, 1),\n",
    "            nn.ConvTranspose1d(hidden, hidden, 4, 2, 1),\n",
    "            nn.ConvTranspose1d(hidden, 1, 4, 2, 1),\n",
    "        ])\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = z\n",
    "        for deconv in self.deconvs:\n",
    "            h = F.relu(deconv(h))\n",
    "        return h.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f66dd",
   "metadata": {},
   "source": [
    "STFT loss (perceptuelle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ff7f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_loss(x, x_hat):\n",
    "    X = torch.stft(x, n_fft=1024, hop_length=256, return_complex=True)\n",
    "    X_hat = torch.stft(x_hat, n_fft=1024, hop_length=256, return_complex=True)\n",
    "    return (X.abs() - X_hat.abs()).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42843f",
   "metadata": {},
   "source": [
    "Modèle VQ-VAE complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe785de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = AudioEncoder()\n",
    "        self.vq_low = VectorQuantizer()\n",
    "        self.vq_up = VectorQuantizer()\n",
    "        self.decoder = AudioDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_low, h_up = self.encoder(x)\n",
    "        z_low, loss_low = self.vq_low(h_low)\n",
    "        z_up, loss_up = self.vq_up(h_up)\n",
    "        x_hat = self.decoder(z_up)\n",
    "        return x_hat, loss_low + loss_up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93301dd",
   "metadata": {},
   "source": [
    "Génération d’un dataset audio provisoire (DEBUG)\n",
    "\n",
    "Objectif\n",
    "Cette cellule génère un petit dataset artificiel et temporaire, uniquement pour :\n",
    "\n",
    "vérifier que le VQ-VAE s’entraîne\n",
    "\n",
    "débugger l’architecture\n",
    "\n",
    "tester la reconstruction\n",
    "\n",
    "⚠️ Ce dataset n’a aucune valeur sémantique\n",
    "Il sera remplacé plus tard par VGGSound / ESC-50 / etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22f11411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATASET PROVISOIRE (DEBUG) - SANS BRUIT\n",
    "\n",
    "- Généré artificiellement pour valider :\n",
    "    - Chaîne encodeur → VQ → décodeur\n",
    "    - Stabilité des loss\n",
    "    - Shapes et reconstruction audio\n",
    "- Signaux volontairement simples :\n",
    "    - sinusoïdes\n",
    "    - glissando (chirp)\n",
    "⚠️ À NE PAS UTILISER pour des résultats finaux\n",
    "\"\"\"\n",
    "\n",
    "NUM_EXAMPLES = 20       # Petit dataset debug\n",
    "class DebugAudioDataset(Dataset):\n",
    "    def __init__(self, num_examples=NUM_EXAMPLES):\n",
    "        self.num_examples = num_examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Création de la timeline\n",
    "        t = torch.linspace(0, DURATION, NUM_SAMPLES)\n",
    "\n",
    "        # Choix aléatoire du type de signal (sans bruit)\n",
    "        choice = random.choice([\"sine\", \"chirp\"])\n",
    "\n",
    "        if choice == \"sine\":\n",
    "            # Sinusoïde simple\n",
    "            freq = random.uniform(100, 2000)\n",
    "            audio = torch.sin(2 * math.pi * freq * t)\n",
    "\n",
    "        elif choice == \"chirp\":\n",
    "            # Glissando linéaire de f0 à f1\n",
    "            f0 = random.uniform(100, 500)\n",
    "            f1 = random.uniform(1000, 4000)\n",
    "            audio = torch.sin(2 * math.pi * (f0 * t + (f1 - f0) * t**2))\n",
    "\n",
    "        # Normalisation entre -1 et 1\n",
    "        audio = audio / audio.abs().max()\n",
    "\n",
    "        # Retourne un tensor float32\n",
    "        return audio.float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373c96a",
   "metadata": {},
   "source": [
    "Dataset DataLoader\n",
    "\n",
    "Objectif\n",
    "Charger le dataset provisoire exactement comme un vrai dataset audio\n",
    "(VGGSound plus tard), sans changer le reste du code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b16c9842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([8, 64000])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DATALOADER POUR DATASET PROVISOIRE\n",
    "\"\"\"\n",
    "\n",
    "dataset = DebugAudioDataset(NUM_EXAMPLES)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0 \n",
    ")\n",
    "\n",
    "# Test\n",
    "audio_batch = next(iter(dataloader))\n",
    "print(\"Batch shape:\", audio_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c62863",
   "metadata": {},
   "source": [
    "Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4168e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "280bde7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\functional.py:681: UserWarning: A window was not provided. A rectangular window will be applied,which is known to cause spectral leakage. Other windows such as torch.hann_window or torch.hamming_window are recommended to reduce spectral leakage.To suppress this warning and use a rectangular window, explicitly set `window=torch.ones(n_fft, device=<device>)`. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\SpectralOps.cpp:842.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 1120.9595\n",
      "Epoch 1 | Loss 1071.0157\n",
      "Epoch 2 | Loss 1027.5417\n",
      "Epoch 3 | Loss 989.4696\n",
      "Epoch 4 | Loss 951.3610\n",
      "Epoch 5 | Loss 914.2968\n",
      "Epoch 6 | Loss 878.8477\n",
      "Epoch 7 | Loss 846.5333\n",
      "Epoch 8 | Loss 810.1745\n",
      "Epoch 9 | Loss 778.4980\n",
      "Epoch 10 | Loss 745.1718\n",
      "Epoch 11 | Loss 712.9871\n",
      "Epoch 12 | Loss 676.6991\n",
      "Epoch 13 | Loss 647.6391\n",
      "Epoch 14 | Loss 622.5359\n",
      "Epoch 15 | Loss 598.3116\n",
      "Epoch 16 | Loss 580.7785\n",
      "Epoch 17 | Loss 579.5220\n",
      "Epoch 18 | Loss 594.6170\n",
      "Epoch 19 | Loss 647.9504\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for audio in dataloader:\n",
    "        audio = audio.to(DEVICE)\n",
    "\n",
    "        audio_hat, loss_vq = model(audio)\n",
    "        loss_rec = stft_loss(audio, audio_hat)\n",
    "\n",
    "        loss = loss_vq + loss_rec\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} | Loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778bbd92",
   "metadata": {},
   "source": [
    "Vérification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8fdea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère le batch audio et le reconstruit\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    audio = next(iter(dataloader)).to(DEVICE)\n",
    "    audio_hat, _ = model(audio)\n",
    "\n",
    "# Conversion en numpy et ajout d'une dimension channel (1, N)\n",
    "original = audio[0].cpu().numpy()\n",
    "reconstructed = audio_hat[0].cpu().numpy()\n",
    "\n",
    "# Sauvegarde avec soundfile\n",
    "sf.write(\"audio_original.wav\", original, SAMPLE_RATE)\n",
    "sf.write(\"audio_reconstructed.wav\", reconstructed, SAMPLE_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
