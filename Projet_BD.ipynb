{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23bf454b",
   "metadata": {},
   "source": [
    "# Projet Informatique - Comics to Music"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc957ad1",
   "metadata": {},
   "source": [
    "## GANs non-conditionnel\n",
    "\n",
    "Dans le code, en voici les différents éléments : \n",
    " - définition du générateur\n",
    " - définition du discriminateur\n",
    " - entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba38af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a92317",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ''\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8be52",
   "metadata": {},
   "source": [
    "### Générateur (sans conditionnement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e73c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonConditionnalGenerator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''On commence par définir les éléments structurels de notre générateur.\n",
    "        Dans notre cas, on suppose d'abord qu'on veut générer vers un espace latent\n",
    "        (génération conditionnée par la suite par un autre espace latent, plus des\n",
    "        conditions supplémentaires).\n",
    "        Le générateur va augmenter la dimension des données en opérant une déconvolution (Upsample, 2e article cGANs)\n",
    "        Ici, on part d'un vecteur bruit z et on utilise une première couche linéaire pour \n",
    "        le mettre sous format-image en 2D. Dans cet exemple, on veut obtenir des images 28x28, donc on transforme z \n",
    "        en images 7x7, ce qui permettra facilement d'atteindre la dimension finale en réalisant deux Upsample de scale = 2\n",
    "        '''\n",
    "\n",
    "        self.z_dim = z_dim # dimension du vecteur bruit \n",
    "        self.init_dim = 7 # dans cet exemple, on veut partir d'une image 7x7\n",
    "        self.num_ch = 128 # nombre de canaux arbitraire (dépend de l'espace latent, peut être des autres conditions)\n",
    "        self.l1 = nn.Sequential( # première couche linéaire\n",
    "            nn.Linear(z_dim, self.num_ch * self.init_dim * self.init_dim) # on change la dimension du bruit pour pouvoir le \n",
    "                                                                          #reshape en [num_ch] images de dimensions init_dim x init_dim\n",
    "        )\n",
    "\n",
    "        # Réseau convolutionnel pour générer une image 28x28  : G_non_conditionnel : bruit --> espace latent audio\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(self.num_ch),\n",
    "            nn.Upsample(scale_factor=2), # passer de 7x7 à 14x14\n",
    "            nn.Conv2d(self.num_ch, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2), # on passe à 28x28\n",
    "            nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        ######################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conversion du vecteur bruit\n",
    "        output1 = self.l1(x) \n",
    "        # reshape en image\n",
    "        output2 = output1.view(output1.shape[0], self.num_ch, self.init_dim, self.init_dim) \n",
    "        # CNN\n",
    "        output = self.model(output2) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2dd2737",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100 # arbitraire\n",
    "nc_generator = NonConditionnalGenerator(z_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402e731",
   "metadata": {},
   "source": [
    "### Discriminateur (sans conditionnement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c6e6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonConditionnalDiscriminator(nn.Module):\n",
    "    def __init__(self, num_ch):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''Le discriminateur va être entraîner pour différencier les données réelles (espace latent audio)\n",
    "        des données générées (G(z), qui dans la version cGAN seront conditionnées par l'espace latent BD).\n",
    "        En pratique, il ne prend ne prend en entrée que des vrais ou que des faux à la fois.\n",
    "        Fonctionnement de l'entraînement : \n",
    "         - on génère les prédictions D(x_real) --> objectif D(x_real) = 1\n",
    "         - on calcule la loss sur les prédictions réelles : loss_real = loss(D(x_real), 1)\n",
    "         - on génère les prédictions D(x_fake) --> objectif D(x_fake) = 1\n",
    "         - on calcule la loss sur les prédictions réelles : loss_fake = loss(D(x_fake), 1)\n",
    "        Le discriminateur est donc entraîné sur loss_tot = (loss_real + loss_fake)\n",
    "        \n",
    "        Architecture : réseau convolutionnel classique, exemple dans le deuxième article sur les cGANs'''\n",
    "        \n",
    "        self.num_ch = num_ch # nombre de canaux, dépend de l'espace latent audio\n",
    "\n",
    "        #  modèle de convolution classique, on cherche à densifier l'information des images à chaque couche\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(num_ch, 64, kernel_size=4, stride=2, padding=1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 512, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # A la fin, on réduit brusquement le nombre de canaux à 1, pour obtenir une prédiction scalire {0,1}.\n",
    "            # Dans la version cGAN, c'est ici que l'on introduit la condition y !!!\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # prend un batch d'images d'entrée, sort un batch de 1 et de 0\n",
    "        output = self.model(x).view(-1,1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24914252",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ch = 3 # arbitraire\n",
    "nc_discriminator = NonConditionnalDiscriminator(num_ch).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51774a4d",
   "metadata": {},
   "source": [
    "### Entraînement (sans conditionnement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07e1419f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (844242149.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtrain_loader = ???\u001b[39m\n                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Dataset générés à partir de l'espace latent audio\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5dd943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres de l'entraînement\n",
    "\n",
    "lr = 0.0001\n",
    "num_epochs = 50\n",
    "loss_function = nn.BCELoss() # ici, loss commune à D et G, à voir si on peut utiliser une loss différente\n",
    "\n",
    "optimizer_nc_discriminator = torch.optim.Adam(nc_discriminator.parameters(), lr=lr)\n",
    "optimizer_nc_generator = torch.optim.Adam(nc_generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e06bcc47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m n, (real_samples, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_loader\u001b[49m):\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m             \u001b[38;5;66;03m### Entraînement nc_discriminator\u001b[39;00m\n\u001b[32m     12\u001b[39m             \u001b[38;5;66;03m# On récupère les données réelles (reshape pour être sûr qu'elles soient acceptés par nc_D)\u001b[39;00m\n\u001b[32m     13\u001b[39m             real_samples = real_samples.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m28\u001b[39m, \u001b[32m28\u001b[39m)\n\u001b[32m     15\u001b[39m             \u001b[38;5;66;03m# on génère les fake_samples à partir d'un bruit gaussien (pas de condition ici)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Sauvegarde et récupération de G et D \n",
    "if os.path.isfile('discriminator2.pt') and os.path.isfile('generator2.pt'):\n",
    "    nc_discriminator.load_state_dict(torch.load('./discriminator2.pt'))\n",
    "    nc_generator.load_state_dict(torch.load('./generator2.pt'))   \n",
    "\n",
    "else:\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for n, (real_samples, _) in enumerate(train_loader):\n",
    "\n",
    "            ### Entraînement nc_discriminator\n",
    "            # On récupère les données réelles (reshape pour être sûr qu'elles soient acceptés par nc_D)\n",
    "            real_samples = real_samples.view(-1, 1, 28, 28)\n",
    "\n",
    "            # on génère les fake_samples à partir d'un bruit gaussien (pas de condition ici)\n",
    "            z = torch.randn(batch_size, z_dim)\n",
    "            fake_samples = torch.tanh(nc_generator(z))\n",
    "\n",
    "            # Initialisation de la descente de gradient\n",
    "            optimizer_nc_discriminator.zero_grad()\n",
    "\n",
    "            # Loss sur les prédictions \"données réelles\"\n",
    "            predict_real = nc_discriminator(real_samples)\n",
    "            loss_real = loss_function(predict_real, torch.full((batch_size, 1), 1.0))\n",
    "\n",
    "            # Loss sur les prédictions \"données générées\"\n",
    "            predict_fake = nc_discriminator(fake_samples.detach())\n",
    "            loss_fake = loss_function(predict_fake, torch.full((batch_size, 1), 0.0))\n",
    "\n",
    "            # Loss complète\n",
    "            loss_discriminator = (loss_real + loss_fake) / 2\n",
    "\n",
    "            # Descente de gradient et mise à jour des poids w_D\n",
    "            loss_discriminator.backward()\n",
    "            optimizer_nc_discriminator.step()\n",
    "\n",
    "            ### Entraînement nc_generator\n",
    "            # Initialisation descente de gradient\n",
    "            optimizer_nc_generator.zero_grad()\n",
    "\n",
    "            # génération des fake_samples\n",
    "            z = torch.randn(batch_size, 100)\n",
    "            fake_samples_new = nc_generator(z)\n",
    "\n",
    "            # récupération de la prédiction de D\n",
    "            gen_prediction = nc_discriminator(fake_samples_new)\n",
    "\n",
    "            # loss classique entre la prédiction D(fake_samples) et l'objectid vecteur de 1\n",
    "            valid_labels = torch.full((batch_size, 1), 1.0)\n",
    "            loss_generator = loss_function(gen_prediction, valid_labels)\n",
    "\n",
    "            # OPTIONNEL --> diversity loss\n",
    "            # on ajoute une diversity loss pour éviter de générer que des points identiques\n",
    "            # sinon le générateur peut mapper à chaque fois vers une seule image qui à l'air vraie\n",
    "            distances = torch.cdist(fake_samples_new, fake_samples_new, p=2) # renvoie la distance scalaire entre tous les éléments générés du batch\n",
    "            mean_distance = torch.mean(distances)\n",
    "            lambda_div = 0.2 # poids sur la loss\n",
    "            loss_diversity = -lambda_div * mean_distance # on veut maximiser la distance donc on met un - devant\n",
    "\n",
    "            # loss totale \n",
    "            loss_generator = loss_generator + loss_diversity\n",
    "            \n",
    "            loss_generator.backward()\n",
    "            optimizer_nc_generator.step()\n",
    "\n",
    "            # Show loss\n",
    "            if n == batch_size - 2:\n",
    "                print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "                print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb370f9",
   "metadata": {},
   "source": [
    "## GANs conditionnels\n",
    "\n",
    "Pas si différents des GANs non-conditionnels, il faut juste rajouter une condition $y$ à deux étapes du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "766dae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionnalGenerator(nn.Module):\n",
    "    def __init__(self, z_dim, y_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.z_dim = z_dim # dimension du vecteur bruit \n",
    "        self.y_dim = y_dim # dimension du vecteur condition\n",
    "        self.init_dim = 7 # dans cet exemple, on veut partir d\"'une image 7x7\n",
    "        self.num_ch = 128 # nombre de canaux arbitraire (dépend de l'espace latent, peut être des autres conditions)\n",
    "        \n",
    "        self.l1 = nn.Sequential( # première couche linéaire\n",
    "            nn.Linear(self.z_dim + self.y_dim, self.num_ch * self.init_dim * self.init_dim) # cette fois, on part de \n",
    "                                                                                            # z_dim + y_dim\n",
    "        )\n",
    "\n",
    "        # Réseau convolutionnel identique au vanilla GAN\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(self.num_ch),\n",
    "            nn.Upsample(scale_factor=2), # passer de 7x7 à 14x14\n",
    "            nn.Conv2d(self.num_ch, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2), # on passe à 28x28\n",
    "            nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        ######################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Ici, x est la concaténation du bruit z et de la condition y\n",
    "        # conversion du vecteur x\n",
    "        output1 = self.l1(x) \n",
    "        # reshape en image\n",
    "        output2 = output1.view(output1.shape[0], self.num_ch, self.init_dim, self.init_dim) \n",
    "        # CNN\n",
    "        output = self.model(output2) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f58fc821",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100 # arbitraire\n",
    "y_dim = 100 # arbitraire\n",
    "c_generator = ConditionnalGenerator(z_dim, y_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3b99924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionnalDiscriminator(nn.Module):\n",
    "    def __init__(self, num_ch, latent_x_dim, y_cond_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.num_ch = num_ch # nombre de canaux, dépend de l'espace latent audio\n",
    "        self.y_cond_dim = y_cond_dim\n",
    "        self.stride_size = 2 # valeur du stride pour connaître la dimension lors du reshape\n",
    "\n",
    "        #  modèle de convolution classique, on cherche à densifier l'information des images à chaque couche\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(num_ch, 64, kernel_size=4, stride=2, padding=1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 512, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        # A cette étape, il faut applatir les données et y concaténer la condition y\n",
    "        # calcul de la dimension du reshape :\n",
    "        self.image_dim = int(latent_x_dim / (self.stride_size ** 3)) # on a appliqué 3 strides de 2\n",
    "        self.flat_dim = 512 * self.image_dim * self.image_dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flat_dim + self.y_cond_dim, 1), # ce n'est plus une convolution car vecteur plat\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # on rentre x et y car on commence par traiter seulement x avant d'injecter y\n",
    "        features = self.model(x)\n",
    "        \n",
    "        # Aplatissement vers la flat dimension\n",
    "        features_flat = features.view(features.size(0), -1) \n",
    "        \n",
    "        # Concaténation avec le vecteur condition y\n",
    "        combined = torch.cat([features_flat, y], dim=1)\n",
    "        \n",
    "        # Prédiction finale\n",
    "        output = self.classifier(combined)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63230332",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ch = 3 # arbitraire\n",
    "x_dim = 16 # arbitraire\n",
    "y_cond_dim = 28 # arbitraire\n",
    "c_discriminator = ConditionnalDiscriminator(num_ch, x_dim, y_cond_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba0ab4",
   "metadata": {},
   "source": [
    "## GANs Cycle-Consistent\n",
    "\n",
    "Dans notre cas, on veut que le GAN apprennent à générer des données dans l'espace latent $Y$ (audio) en étant influencé par l'espace latent $X$ (BDs). Pour cela, deux solutions : entraîner notre GAN avec des données réelles pairées, en conservant les paires dans les espaces latents $X$ et $Y$. Problème : auncun dataset existant possédant des paires d'audio et de BDs. On va donc chercher à entraîner notre GAN pour qu'il puisse associer des échantillons $x\\in \\{x_i\\}$ à des échantillons $y\\in\\{y_i\\}$, de manière non supervisée, en détectant lui-même les caractéristiques pour associer une paire. On s'inspire de l'article *Unpaired Image-to-Image Translation\n",
    "using Cycle-Consistent Adversarial Networks*, qui introduisent les *Cycle-Consistent GANs*. Ils utilisent ce réseau pour passer d'images à images. Dans notre cas, les espaces $X$ et $Y$ ne sont pas aussi similaires, il va peut être falloir injecter des conditions supplémentaires (structure, rythme, couleur, texte, sens --> CLIP), pour aider notre GAN à générer des paires qui ont du sens.\n",
    "\n",
    "En pratique, on a besoin d'entraîner deux générateurs\n",
    "$$\n",
    "G : X \\longrightarrow Y \\\\\n",
    "F : Y \\longrightarrow X\n",
    "$$\n",
    "\n",
    "Chacun associé à un discrimnateur, qui va déterminer si la sortie du générateur correspond à l'espace réel cible. Là génération est donc entraînée par une adversarial loss. On s'inspire des travaux de *Improved Training of Wasserstein GANs* qui ont introduit les WGANs, utilisant une distance de Wasserstein comme adversarial loss. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{WGAN} (G, D_Y, X, Y) =\\: &\\mathbb{E}_{y\\sim p_{data}(y)}[D_Y(y)]\\\\ - \\: &\\mathbb{E}_{x\\sim p_{data}(x)}[D_Y(G(x))]\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "Contrairement à un GAN classique, on ne cherchera à prédire des 1 ou des 0. On va ici chercher à maximiser les écarts entre les score vrais ou faux. Nos discriminateurs sortent donc des vecteurs de score, et non pas des labels de prédiction. On en tire une loss pour les discriminateurs, et une pour les générateurs : \n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_D &= \\mathbb{E}[D(G(x))] - \\mathbb{E}[D(y)] + \\lambda_{GP}\\mathcal{L}_{GP}\\\\\n",
    "\\mathcal{L}_G &= -\\mathbb{E}[D(G(x))]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "où $\\mathcal{L}_{GP}$ est la $gradient \\: penalty$ introduite dans l'article WGANs (ils utilisent $\\lambda_{GP}=10$). Cette pénalité vise à empêcher la sortie du discriminateur d'avoir une norme différente de 1. Ils l'expriment comme : \n",
    "$$\n",
    "\\mathcal{L}_{GP} = \\mathbb{E}[(||\\nabla_{\\hat{y} = G(x)}D(G(x))||_2 -1)^2]\n",
    "$$\n",
    "Un aspect important de l'article est qu'il stipule que le discriminateur doit être entraîné plus souvent que le générateur. Environ 5 steps pour 1.\n",
    "\n",
    "\n",
    "L'architecture globale *Cycle-GANs* nécessite d'ajouter une *cycle-loss*, qui détermine si les données sont bien reconstruites lors de l'application successive de $G$ et $F$ :\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{cyc} =\\: &\\mathbb{E}_{x\\sim p_{data}(x)}[||F(G(x)) - x||]\\\\ + \\:&\\mathbb{E}_{y\\sim p_{data}(y)}[||G(F(y)) - y||]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Pour l'architecture, l'article *Cycle-GANs* s'inspire de Johnson et al. *Perceptual losses\n",
    "for real-time style transfer and super-resolution*. On y ajoute la notion développée dans l'article *LOGAN : Unpaired Shape Transform in Latent Overcomplete Space*, qui présente l'avantage de travailler avec des espaces *overcomplete* (réseaux dans lesquels la dimension a été fortement augmentée). Cela facilite le transport de masse optimale entre les deux distributions (--> coût égal *masse* x *distance(X,Y)*). Cet ajout agit avec la Wassertein-Loss pour empêcher le *mode-collapse* : \n",
    "\n",
    "1. Générateurs :\n",
    " - $\\longrightarrow$ donc pour ces GANs là, on n'augmente pas forcément la dimension comme vu dans les cGANs (on peut quand même modifier si jamais nos espaces $X$ et $Y$ n'ont pas les mêmes dimensions). Nos espaces latents auront peut être des dimensions différentes (par exemple : peut être besoin de plus grandes dimensions dans l'espace latent audio, si on utilise plein de conditions supplémentaires dans l'espace BD). \n",
    "\n",
    " 2. Discriminateurs :\n",
    " - $N\\times N$ Patch-GANs : faits pour discriminer des grosses images (1080p par exemple). Ils ne discriminent que sur des portions de $N$ par $N$. Peut être pas nécessaire pour nous si on travaille sur des espaces latents de faibles dimensions. Mais peux justement permettre de travailler sur des espaces latents plus complexes sans trop augmenter la charge de calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e036cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples, device=device):\n",
    "    \"\"\"Calcul du Gradient Penalty pour WGAN-GP\"\"\"\n",
    "    \n",
    "    # On adapte les dimensions de alpha pour qu'elles collent aux données (1D ou 2D)\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device) if len(real_samples.shape) == 4 else torch.rand(real_samples.size(0), 1, device=device)\n",
    "    \n",
    "    # si c'est un vecteur 1D (X), alpha doit être (Batch, 1)\n",
    "    # si c'est un tensuer 2D (Y), alpha doit être (Batch, 1, 1, 1) pour le broadcast\n",
    "    if len(real_samples.shape) == 2:\n",
    "         alpha = alpha.view(real_samples.size(0), 1)\n",
    "    \n",
    "    # Interpolation\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    \n",
    "    # Passage dans le discriminateur\n",
    "    d_interpolates = D(interpolates)\n",
    "    \n",
    "    # Calcul du gradient\n",
    "    # On crée un vecteur de 1 --> cible de notre gradient\n",
    "    fake = torch.ones(d_interpolates.size(), device=device, requires_grad=False)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    \n",
    "    # Calcul de la pénalité\n",
    "    gradients = gradients.view(gradients.size(0), -1) # on aplatit pour le calcul de norme\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834b2ad",
   "metadata": {},
   "source": [
    "### Générateur $G$ : $X$(BDs) $\\longrightarrow$ $Y$(audios)\n",
    "\n",
    "$G$ part de $X$ l'espace latent BDs pour générer $Y$ l'espace latent audio. Comment on utilisera des conditions supplémentaires pour influencer la génération, on peut se permettre de ne garder que peut d'information dans $X$. Ainsi, on choisit de générer $X$ (partie VAE) comme un vecteur 1D plutôt que comme une image. On ne garde ainsi qu'une idée du style et on oublie les informations de structure spatiale qui seront utilisées plus efficacement dans les autres conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0fbf36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_G(nn.Module):\n",
    "    def __init__(self, z_dim, latent_x_dim, x_ch, init_dim):\n",
    "        '''\n",
    "        Inputs :\n",
    "         - z_dim : dimension du vecteur bruit seed du générateur\n",
    "        Dimensions des espaces latents: \n",
    "         - latent_x_dim : dimension de l'espace latent X d'où est tirée la condition du générateur G.\n",
    "            On peut générer l'espace latent BDs comme un vecteur, car on ne veut l'utiliser que pour avoir une \n",
    "            notion du style\n",
    "         - pas besoin de prendre latent_y_dim pour connaître la dimension de l'objectif si l'architecture\n",
    "         est adaptée en fonction de la dimension de départ init_dim (voir juste en dessous)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim # dimension du vecteur bruit \n",
    "\n",
    "        # dimensions des espaces X et Y (on suppose dim(X) < dim(Y))\n",
    "        # G opère donc une déconvolution pour passer de X à Y\n",
    "        self.x_dim = latent_x_dim # dimension du vecteur condition (espace BDs X)\n",
    "\n",
    "        self.init_dim  = init_dim # dimension (H ou W) de l'image 2D à l'entrée du CNN\n",
    "        # idéalement dim(Y) = dim_init * 2**(n_upsample) pour qu'on puisse facilement arriver à la bonne dimension (ici n_upsample = 2) \n",
    "        self.num_ch = x_ch # nombre de canaux arbitraire (dépend de l'espace latent qu'on choisit, peut être des autres conditions aussi)\n",
    "        \n",
    "        # Première couche linéaire\n",
    "        self.l1 = nn.Sequential( \n",
    "            nn.Linear(self.z_dim + self.x_dim, self.num_ch * self.init_dim * self.init_dim) \n",
    "        )\n",
    "\n",
    "        # Réseau convolutionnel --> déconvolution d'une petite dimension vers une grande dimension\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(self.num_ch),\n",
    "            nn.Upsample(scale_factor=2), # (init_dim)x(init_dim) --> (2*init_dim)x(2*init_dim)\n",
    "            nn.Conv2d(self.num_ch, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2), # (2*init_dim)x(2*init_dim) --> (4*init_dim)x(4*init_dim)\n",
    "            nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        # Concaténation sur la dimension 1\n",
    "        cat_xz = torch.cat([x, z], dim=1) \n",
    "        output1 = self.l1(cat_xz) \n",
    "        \n",
    "        output2 = output1.view(output1.shape[0], self.num_ch, self.init_dim, self.init_dim) \n",
    "\n",
    "        output = self.model(output2) \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a74592dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100 # arbitraire\n",
    "x_dim = 100 # dimension d'un vecteur\n",
    "x_ch = 32\n",
    "y_dim = 64 # arbitraire, tenseur 2D 64x64\n",
    "init_dim = 64 // (2**2)\n",
    "G = Generator_G(z_dim, x_dim, x_ch, init_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e9253",
   "metadata": {},
   "source": [
    "### Générateur $F$ : $Y$(audios) $\\longrightarrow$ $X$(BDs)\n",
    "\n",
    "Contrairement à $G$, qui développait les données, $F$ a pour but de compresser l'espace $Y$ vers $X$. On veut que les deux générateurs forment un équilibre lors de l'apprentissage. On a également les même dimensions que dans $G$, mais inversées. Il es dont logique que l'architecture de $F$ soit symétrique à celle de $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef4d7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_F(nn.Module):\n",
    "    def __init__(self, z_dim, latent_x_dim, x_ch, y_ch, init_dim):\n",
    "        '''\n",
    "        Inputs :\n",
    "         - latent_x_dim : dimension du vecteur de sortie (espace X)\n",
    "         - latent_y_dim : nombre de canaux de l'espace latent d'entrée Y\n",
    "         - z_dim        : dimension du bruit gaussien ajouté\n",
    "         - init_dim     : dimension spatiale de référence\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.x_dim = latent_x_dim\n",
    "        self.y_ch = y_ch\n",
    "        self.z_dim = z_dim     \n",
    "        self.init_dim = init_dim\n",
    "        self.internal_ch = x_ch\n",
    "\n",
    "        # Le réseau prend maintenant en entrée canaux de Y + canaux de Z\n",
    "        input_channels = self.y_ch + self.z_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(input_channels, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # equivalent à un Downsample\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2), \n",
    "\n",
    "            nn.Conv2d(256, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, self.internal_ch, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(self.internal_ch),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.flat_dim = self.internal_ch * self.init_dim * self.init_dim\n",
    "        \n",
    "        self.final_head = nn.Sequential(\n",
    "            nn.Linear(self.flat_dim, self.x_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_y, z):\n",
    "\n",
    "        # comme on ajoute z à une matrice 2D, il faut l'étendre (comme la condition X dans DY)\n",
    "        batch_size, _, h, w = input_y.size()\n",
    "        z_expanded = z.view(batch_size, -1, 1, 1).expand(batch_size, -1, h, w)\n",
    "        combined_input = torch.cat([input_y, z_expanded], dim=1)\n",
    "        \n",
    "        features = self.model(combined_input)\n",
    "        features_flat = features.view(features.shape[0], -1)\n",
    "        output_x = self.final_head(features_flat)\n",
    "        \n",
    "        return output_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fd539a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100 # arbitraire\n",
    "x_dim = 100 # dimension d'un vecteur\n",
    "x_ch = 32\n",
    "y_ch = 64 # arbitraire, tenseur 2D 64x64\n",
    "init_dim = 64 // (2**2)\n",
    "F = Generator_F(z_dim, x_dim, x_ch, x_ch, init_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d4ea8",
   "metadata": {},
   "source": [
    "### Discriminateur $D_Y$\n",
    "\n",
    "$D_Y$ vérifie que $G(x) = \\hat{y} \\approx y$. \n",
    "\n",
    "**Au final les discriminateurs ne prennent pas de conditions. $\\\\$ EXPLICATION :  la loss des discriminateurs est calculée d'abord sur des prédictions à partir d'entrées totalement vraies, puis sur des prédictions à partir d'entrées totalement fausses (ou inversement) le conditionnement fonctionne pour la prédiction à partir de données fausses, puisque l'on veut que  les données générées depuis l'autre espace soit notées en fonction de l'autre espace. Cependant, pour les prédictions à partir de données réelles, comme les données ne sont pas pairées, la conditions ne veut rien dire pour le discriminateur. Il ne veut donc même pas reconnaître ça comme une donnée réelle lors du calcul de la loss (à clarifier).**\n",
    "\n",
    "**Du coup, *Comment assurer que le modèle découvre une cohérence entre X et Y ?* On s'appuie pour cela sur la Cycle-loss, qui permet de retrouver les données d'un espace en appliquant les deux générations successives. Cette Loss oblige le modèle à lier les échantillons $x$ et $y$ qui lui permettront de reproduire le plus fidèlement les espaces à partir des générations (à clarifier)**. \n",
    "\n",
    "Le discriminateur va donc prendre en entrée des données 2D, on peut donc utiliser une structure patchGAN $N\\times N$, qui permettra de réduire la quantité de calculs de la discrimination, et d'avoir un espace latent $Y$ plus complexe.\n",
    "\n",
    "Un discriminateur de type PatchGAN ne sort pas un scalaire de prédiction mais une grille de prédiction pour chaque patche de données. La couche linéaire à la fin du discriminateur conditionnel vu juste avant doit donc nécessairement disparaître car on veut une sortie 2D pour le discriminateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd47a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_DY(nn.Module):\n",
    "    def __init__(self, y_ch, ndf=64):\n",
    "        \"\"\"\n",
    "        y_ch : Canaux de l'image Y\n",
    "        ndf  : Nombre de filtres de base\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Layer 1 : Y -> Features\n",
    "            nn.Conv2d(y_ch, ndf, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # Layer 4 : Sortie PatchGAN (Grille 2D)\n",
    "            nn.Conv2d(ndf * 4, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, y_input):\n",
    "        # Plus besoin de x_condition\n",
    "        return self.model(y_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cc6cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 100 # dimension d'un vecteur\n",
    "num_ch_y = 3 # arbitraire\n",
    "DY = Discriminator_DY(num_ch_y).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f08ce",
   "metadata": {},
   "source": [
    "### Discriminateur $D_X$\n",
    "\n",
    "Ce discriminateur agit sur l'espace $X$. Les données sont donc 1D, et plus simples que pour $D_Y$, on n'a pas besoin d'utiliser un discriminateur de type PatchGAN. On utilise un discriminateur conditionnée classique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_DX(nn.Module):\n",
    "    def __init__(self, x_ch, latent_x_dim, ndf=64):\n",
    "        \"\"\"\n",
    "        x_ch         : Nombre de canaux de l'espace latent X (input)\n",
    "        latent_x_dim : Dimension spatiale (H ou W) de l'entrée X\n",
    "                       (Utilisé pour calculer la taille du vecteur aplati)\n",
    "        ndf          : Nombre de filtres de base\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.x_ch = x_ch \n",
    "        self.stride_size = 2 \n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(self.x_ch, 64, kernel_size=4, stride=2, padding=1), \n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(128), \n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(128, 512, kernel_size=3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(512), \n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        # Calcul de la dimension spatiale finale avant aplatissement\n",
    "        # On divise par 8 car il y a 3 couches avec stride 2 (2^3 = 8)\n",
    "        self.final_spatial_dim = int(latent_x_dim / 8)\n",
    "        \n",
    "        # Dimension du vecteur aplati\n",
    "        self.flat_dim = 512 * self.final_spatial_dim * self.final_spatial_dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Plus de cond_dim ici, seulement les features extraites\n",
    "            nn.Linear(self.flat_dim, 1),\n",
    "            #nn.Sigmoid() # pas de Sigmoid pour un WGAN\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        features = self.model(x)\n",
    "        features_flat = features.view(features.size(0), -1) \n",
    "        output = self.classifier(features_flat)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0322754",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 100 # dimension d'un vecteur\n",
    "y_dim = 28 # arbitraire, 28x28\n",
    "num_ch_x = 3 # arbitraire\n",
    "DX = Discriminator_DX(num_ch_x, x_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14825109",
   "metadata": {},
   "source": [
    "### Entraînement du modèle\n",
    "\n",
    "Pour entraîner nos deux générateurset nos discriminateurs, il faut successivement générerles données, et les prédictions et calculer les loss de chaque sous-structure à chaque itération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67badd16",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1530591102.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtrain_loader_X = ???\u001b[39m\n                     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Datasets générés à partir de l'espace latent audio\n",
    "\n",
    "batch_size = 32\n",
    "train_loader_X = ???\n",
    "train_loader_Y = ???\n",
    "iter_Y = iter(train_loader_Y) # si train_loader_Y est plus petit, on itère quand on arrive à la fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cff9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres de l'entraînement\n",
    "\n",
    "lr = 0.0001\n",
    "num_epochs = 50\n",
    "lambda_cycle = 0.2 # poids de la cycle_loss\n",
    "lambda_gp = 10 # poids de la gradient penalty\n",
    "n_critic = 5 # ration entraînement discriminateurs/générateurs\n",
    "b1 = 0.0  # paramètres ADAM recommandés par le papier\n",
    "b2 = 0.9\n",
    "\n",
    "# Losses\n",
    "# les losses des générateurs et discriminateurs pour un WGAN font simplement intervenir des torch.mean\n",
    "loss_func_cycle = nn.BCELoss() \n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(b1,b2))\n",
    "optimizer_F = torch.optim.Adam(F.parameters(), lr=lr, betas=(b1,b2))\n",
    "optimizer_DY = torch.optim.Adam(DY.parameters(), lr=lr, betas=(b1,b2))\n",
    "optimizer_DX = torch.optim.Adam(DX.parameters(), lr=lr, betas=(b1,b2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8760a786",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, real_x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_loader_X\u001b[49m):\n\u001b[32m      4\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m             real_y = \u001b[38;5;28mnext\u001b[39m(iter_Y)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader_X' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, real_x in enumerate(train_loader_X):\n",
    "        \n",
    "        try:\n",
    "            real_y = next(iter_Y)\n",
    "        except StopIteration:\n",
    "            # Si on a fini le dataset Y, on le recharge\n",
    "            iter_Y = iter(train_loader_Y)\n",
    "            real_y = next(iter_Y)\n",
    "\n",
    "        real_x = real_x.to(device)\n",
    "        real_y = real_y.to(device)\n",
    "        \n",
    "        # on s'assure qu'ils aient les bonnes dimensions\n",
    "        real_x = ...\n",
    "        real_y = ...\n",
    "\n",
    "        with torch.no_grad(): # on ne veut que les gradients des générateurs soient calculés pour l'instant\n",
    "            z_x = torch.randn(batch_size, 100).to(device)\n",
    "            fake_x = F(real_y, z_y)\n",
    "            z_y = torch.randn(batch_size, 100).to(device)\n",
    "            fake_y = G(real_x, z_x)\n",
    "\n",
    "        ### ENTRAINEMENT DISCRIMINATEURS\n",
    "        optimizer_DY.zero_grad()\n",
    "        optimizer_DX.zero_grad()\n",
    "        \n",
    "        # Discriminateur DY\n",
    "        score_real_Y = DY(real_y) # les discriminateurs sortent des scores, pas des prédictions\n",
    "        score_fake_Y = DY(fake_y.detach()) # detach() --> on ne veut pas que le gradient remonte au générateur\n",
    "        # Loss WGAN : E[D(fake)] - E[D(real)] + lambda * GP\n",
    "        loss_DY = torch.mean(score_fake_Y) - torch.mean(score_real_Y) + lambda_gp * compute_gradient_penalty(DY, real_y,fake_y.detach())\n",
    "        loss_DY.backward()\n",
    "        optimizer_DY.step()\n",
    "\n",
    "        # Discriminateur DX\n",
    "        score_real_X = DX(real_x) # les discriminateurs sortent des scores, pas des prédictions\n",
    "        score_fake_X = DY(fake_x.detach()) # detach() --> on ne veut pas que le gradient remonte au générateur\n",
    "        # Loss WGAN : E[D(fake)] - E[D(real)] + lambda * GP\n",
    "        loss_DX = torch.mean(score_fake_X) - torch.mean(score_real_X) + lambda_gp * compute_gradient_penalty(DX, real_x,fake_x.detach())\n",
    "        loss_DX.backward()\n",
    "        optimizer_DX.step()\n",
    "\n",
    "        ### ENTRAINEMENT GENERATEURS\n",
    "        if i % n_critic:\n",
    "            optimizer_G.zero_grad()\n",
    "            optimizer_F.zero_grad()\n",
    "\n",
    "            z_x = torch.randn(batch_size, 100).to(device)\n",
    "            fake_x = F(real_y, z_y)\n",
    "            z_y = torch.randn(batch_size, 100).to(device)\n",
    "            fake_y = G(real_x, z_x)\n",
    "\n",
    "            # Calcul des GAN-Loss --> tromper le discriminateur\n",
    "            # Le générateur veut maximiser D(fake), donc minimiser -D(fake)\n",
    "            predict_fake_Y = DY(fake_y)\n",
    "            Loss_GAN_G = -torch.mean(predict_fake_Y)\n",
    "            predict_fake_X = DX(fake_x)\n",
    "            Loss_GAN_F = -torch.mean(predict_fake_X)\n",
    "\n",
    "            # Calcul de Cycle-Loss --> pour revenir à l'espace de départ\n",
    "            reconstructed_x = F(fake_y, torch.randn_like(z_y))\n",
    "            Loss_cycle_X = loss_func_cycle(reconstructed_x, real_x)\n",
    "            reconstructed_y = G(fake_x, torch.randn_like(z_x))\n",
    "            Loss_cycle_Y = loss_func_cycle(reconstructed_y, real_y)\n",
    "\n",
    "            # Loss totale\n",
    "            Loss_Generator = Loss_GAN_G + Loss_GAN_F + lambda_cycle*(Loss_cycle_X + Loss_cycle_Y)\n",
    "            Loss_Generator.backward()\n",
    "            optimizer_G.step()\n",
    "            optimizer_F.step()\n",
    "\n",
    "        # Show loss\n",
    "        if n == batch_size - 2:\n",
    "            print(f\"Epoch: {epoch} Loss G : {Loss_GAN_G}\")\n",
    "            print(f\"Epoch: {epoch} Loss F : {Loss_GAN_F}\")\n",
    "            print(f\"Epoch: {epoch} Loss DY : {loss_DY}\")\n",
    "            print(f\"Epoch: {epoch} Loss DX : {loss_DX}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
