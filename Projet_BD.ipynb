{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23bf454b",
   "metadata": {},
   "source": [
    "# Projet Informatique - Comics to Music"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc957ad1",
   "metadata": {},
   "source": [
    "## GANs non-conditionnel\n",
    "\n",
    "Dans le code, en voici les différents éléments : \n",
    " - définition du générateur\n",
    " - définition du discriminateur\n",
    " - entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba38af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a92317",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ''\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8be52",
   "metadata": {},
   "source": [
    "### Générateur (sans conditionnement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e73c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonConditionnalGenerator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''On commence par définir les éléments structurels de notre générateur.\n",
    "        Dans notre cas, on suppose d'abord qu'on veut générer vers un espace latent\n",
    "        (génération conditionnée par la suite par un autre espace latent, plus des\n",
    "        conditions supplémentaires).\n",
    "        Le générateur va augmenter la dimension des données en opérant une déconvolution (Upsample, 2e article cGANs)\n",
    "        Ici, on part d'un vecteur bruit z et on utilise une première couche linéaire pour \n",
    "        le mettre sous format-image en 2D. Dans cet exemple, on veut obtenir des images 28x28, donc on transforme z \n",
    "        en images 7x7, ce qui permettra facilement d'atteindre la dimension finale en réalisant deux Upsample de scale = 2\n",
    "        '''\n",
    "\n",
    "        self.z_dim = z_dim # dimension du vecteur bruit \n",
    "        self.init_dim = 7 # dans cet exemple, on veut partir d'une image 7x7\n",
    "        self.num_ch = 128 # nombre de canaux arbitraire (dépend de l'espace latent, peut être des autres conditions)\n",
    "        self.l1 = nn.Sequential( # première couche linéaire\n",
    "            nn.Linear(z_dim, self.num_ch * self.init_dim * self.init_dim) # on change la dimension du bruit pour pouvoir le \n",
    "                                                                          #reshape en [num_ch] images de dimensions init_dim x init_dim\n",
    "        )\n",
    "\n",
    "        # Réseau convolutionnel pour générer une image 28x28  : G_non_conditionnel : bruit --> espace latent audio\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(self.num_ch),\n",
    "            nn.Upsample(scale_factor=2), # passer de 7x7 à 14x14\n",
    "            nn.Conv2d(self.num_ch, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2), # on passe à 28x28\n",
    "            nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        ######################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conversion du vecteur bruit\n",
    "        output1 = self.l1(x) \n",
    "        # reshape en image\n",
    "        output2 = output1.view(output1.shape[0], self.num_ch, self.init_dim, self.init_dim) \n",
    "        # CNN\n",
    "        output = self.model(output2) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2dd2737",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100 # arbitraire\n",
    "nc_generator = NonConditionnalGenerator(z_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402e731",
   "metadata": {},
   "source": [
    "### Discriminateur (sans conditionnement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c6e6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonConditionnalDiscriminator(nn.Module):\n",
    "    def __init__(self, num_ch):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''Le discriminateur va être entraîner pour différencier les données réelles (espace latent audio)\n",
    "        des données générées (G(z), qui dans la version cGAN seront conditionnées par l'espace latent BD).\n",
    "        En pratique, il ne prend ne prend en entrée que des vrais ou que des faux à la fois.\n",
    "        Fonctionnement de l'entraînement : \n",
    "         - on génère les prédictions D(x_real) --> objectif D(x_real) = 1\n",
    "         - on calcule la loss sur les prédictions réelles : loss_real = loss(D(x_real), 1)\n",
    "         - on génère les prédictions D(x_fake) --> objectif D(x_fake) = 1\n",
    "         - on calcule la loss sur les prédictions réelles : loss_fake = loss(D(x_fake), 1)\n",
    "        Le discriminateur est donc entraîné sur loss_tot = (loss_real + loss_fake)\n",
    "        \n",
    "        Architecture : réseau convolutionnel classique, exemple dans le deuxième article sur les cGANs'''\n",
    "        \n",
    "        self.num_ch = num_ch # nombre de canaux, dépend de l'espace latent audio\n",
    "\n",
    "        #  modèle de convolution classique, on cherche à densifier l'information des images à chaque couche\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(num_ch, 64, kernel_size=4, stride=2, padding=1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 512, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # A la fin, on réduit brusquement le nombre de canaux à 1, pour obtenir une prédiction scalire {0,1}.\n",
    "            # Dans la version cGAN, c'est ici que l'on introduit la condition y !!!\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # prend un batch d'images d'entrée, sort un batch de 1 et de 0\n",
    "        output = self.model(x).view(-1,1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24914252",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ch = 3 # arbitraire\n",
    "nc_discriminator = NonConditionnalDiscriminator(num_ch).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51774a4d",
   "metadata": {},
   "source": [
    "### Entraînement (sans conditionnement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07e1419f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (844242149.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtrain_loader = ???\u001b[39m\n                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Dataset générés à partir de l'espace latent audio\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb5dd943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres de l'entraînement\n",
    "\n",
    "lr = 0.0001\n",
    "num_epochs = 50\n",
    "loss_function = nn.BCELoss() # ici, loss commune à D et G, à voir si on peut utiliser une loss différente\n",
    "\n",
    "optimizer_nc_discriminator = torch.optim.Adam(nc_discriminator.parameters(), lr=lr)\n",
    "optimizer_nc_generator = torch.optim.Adam(nc_generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e06bcc47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m n, (real_samples, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_loader\u001b[49m):\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m             \u001b[38;5;66;03m### Entraînement nc_discriminator\u001b[39;00m\n\u001b[32m     12\u001b[39m             \u001b[38;5;66;03m# On récupère les données réelles (reshape pour être sûr qu'elles soient acceptés par nc_D)\u001b[39;00m\n\u001b[32m     13\u001b[39m             real_samples = real_samples.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m28\u001b[39m, \u001b[32m28\u001b[39m)\n\u001b[32m     15\u001b[39m             \u001b[38;5;66;03m# on génère les fake_samples à partir d'un bruit gaussien (pas de condition ici)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Sauvegarde et récupération de G et D \n",
    "if os.path.isfile('discriminator2.pt') and os.path.isfile('generator2.pt'):\n",
    "    nc_discriminator.load_state_dict(torch.load('./discriminator2.pt'))\n",
    "    nc_generator.load_state_dict(torch.load('./generator2.pt'))   \n",
    "\n",
    "else:\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for n, (real_samples, _) in enumerate(train_loader):\n",
    "\n",
    "            ### Entraînement nc_discriminator\n",
    "            # On récupère les données réelles (reshape pour être sûr qu'elles soient acceptés par nc_D)\n",
    "            real_samples = real_samples.view(-1, 1, 28, 28)\n",
    "\n",
    "            # on génère les fake_samples à partir d'un bruit gaussien (pas de condition ici)\n",
    "            z = torch.randn(batch_size, z_dim)\n",
    "            fake_samples = torch.tanh(nc_generator(z))\n",
    "\n",
    "            # Initialisation de la descente de gradient\n",
    "            optimizer_nc_discriminator.zero_grad()\n",
    "\n",
    "            # Loss sur les prédictions \"données réelles\"\n",
    "            predict_real = nc_discriminator(real_samples)\n",
    "            loss_real = loss_function(predict_real, torch.full((batch_size, 1), 1.0))\n",
    "\n",
    "            # Loss sur les prédictions \"données générées\"\n",
    "            predict_fake = nc_discriminator(fake_samples.detach())\n",
    "            loss_fake = loss_function(predict_fake, torch.full((batch_size, 1), 0.0))\n",
    "\n",
    "            # Loss complète\n",
    "            loss_discriminator = (loss_real + loss_fake) / 2\n",
    "\n",
    "            # Descente de gradient et mise à jour des poids w_D\n",
    "            loss_discriminator.backward()\n",
    "            optimizer_nc_discriminator.step()\n",
    "\n",
    "            ### Entraînement nc_generator\n",
    "            # Initialisation descente de gradient\n",
    "            optimizer_nc_generator.zero_grad()\n",
    "\n",
    "            # génération des fake_samples\n",
    "            z = torch.randn(batch_size, 100)\n",
    "            fake_samples_new = nc_generator(z)\n",
    "\n",
    "            # récupération de la prédiction de D\n",
    "            gen_prediction = nc_discriminator(fake_samples_new)\n",
    "\n",
    "            # loss classique entre la prédiction D(fake_samples) et l'objectid vecteur de 1\n",
    "            valid_labels = torch.full((batch_size, 1), 1.0)\n",
    "            loss_generator = loss_function(gen_prediction, valid_labels)\n",
    "\n",
    "            # OPTIONNEL --> diversity loss\n",
    "            # on ajoute une diversity loss pour éviter de générer que des points identiques\n",
    "            # sinon le générateur peut mapper à chaque fois vers une seule image qui à l'air vraie\n",
    "            distances = torch.cdist(fake_samples_new, fake_samples_new, p=2) # renvoie la distance scalaire entre tous les éléments générés du batch\n",
    "            mean_distance = torch.mean(distances)\n",
    "            lambda_div = 0.2 # poids sur la loss\n",
    "            loss_diversity = -lambda_div * mean_distance # on veut maximiser la distance donc on met un - devant\n",
    "\n",
    "            # loss totale \n",
    "            loss_generator = loss_generator + loss_diversity\n",
    "            \n",
    "            loss_generator.backward()\n",
    "            optimizer_nc_generator.step()\n",
    "\n",
    "            # Show loss\n",
    "            if n == batch_size - 2:\n",
    "                print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "                print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb370f9",
   "metadata": {},
   "source": [
    "## GANs condtionnels\n",
    "\n",
    "Pas si différents des GANs non-conditionnels, il faut juste rajouter une condition $y$ à deux étapes du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "766dae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionnalGenerator(nn.Module):\n",
    "    def __init__(self, z_dim, y_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.z_dim = z_dim # dimension du vecteur bruit \n",
    "        self.y_dim = y_dim # dimension du vecteur condition\n",
    "        self.init_dim = 7 # dans cet exemple, on veut partir d\"'une image 7x7\n",
    "        self.num_ch = 128 # nombre de canaux arbitraire (dépend de l'espace latent, peut être des autres conditions)\n",
    "        \n",
    "        self.l1 = nn.Sequential( # première couche linéaire\n",
    "            nn.Linear(self.z_dim + self.y_dim, self.num_ch * self.init_dim * self.init_dim) # cette fois, on part de \n",
    "                                                                                            # z_dim + y_dim\n",
    "        )\n",
    "\n",
    "        # Réseau convolutionnel identique au vanilla GAN\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(self.num_ch),\n",
    "            nn.Upsample(scale_factor=2), # passer de 7x7 à 14x14\n",
    "            nn.Conv2d(self.num_ch, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2), # on passe à 28x28\n",
    "            nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        ######################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Ici, x est la concaténation du bruit z et de la condition y\n",
    "        # conversion du vecteur x\n",
    "        output1 = self.l1(x) \n",
    "        # reshape en image\n",
    "        output2 = output1.view(output1.shape[0], self.num_ch, self.init_dim, self.init_dim) \n",
    "        # CNN\n",
    "        output = self.model(output2) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f58fc821",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100 # arbitraire\n",
    "y_dim = 100 # arbitraire\n",
    "c_generator = ConditionnalGenerator(z_dim, y_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b99924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionnalDiscriminator(nn.Module):\n",
    "    def __init__(self, num_ch, latent_x_dim, y_cond_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.num_ch = num_ch # nombre de canaux, dépend de l'espace latent audio\n",
    "        self.y_cond_dim = y_cond_dim\n",
    "        self.stride_size = 2 # valeur du stride pour connaître la dimension lors du reshape\n",
    "\n",
    "        #  modèle de convolution classique, on cherche à densifier l'information des images à chaque couche\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(num_ch, 64, kernel_size=4, stride=2, padding=1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 512, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        # A cette étape, il faut applatir les données et y concaténer la condition y\n",
    "        # calcul de la dimension du reshape :\n",
    "        self.image_dim = int(latent_x_dim / (self.stride_size ** 3)) # on a appliqué 3 strides de 2\n",
    "        self.flat_dim = 512 * self.image_dim * self.image_dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flat_dim + self.y_cond_dim, 1), # ce n'est plus une convolution car vecteur plat\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # on rentre x et y car on commence par traiter seulement x avant d'injecter y\n",
    "        features = self.model(x)\n",
    "        \n",
    "        # Aplatissement vers la flat dimension\n",
    "        features_flat = features.view(features.size(0), -1) \n",
    "        \n",
    "        # Concaténation avec le vecteur condition y\n",
    "        combined = torch.cat([features_flat, y], dim=1)\n",
    "        \n",
    "        # Prédiction finale\n",
    "        output = self.classifier(combined)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63230332",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ch = 3 # arbitraire\n",
    "x_dim = 16 # arbitraire\n",
    "y_cond_dim = 28 # arbitraire\n",
    "c_discriminator = ConditionnalDiscriminator(num_ch, x_dim, y_cond_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba0ab4",
   "metadata": {},
   "source": [
    "## GANs Cycle-Consistent\n",
    "\n",
    "Dans notre cas, on veut que le GAN apprennent à générer des données dans l'espace latent $Y$ (audio) en étant influencé par l'espace latent $X$ (BDs). Pour cela, deux solutions : entraîner notre GAN avec des données réelles pairées, en conservant les paires dans les espaces latents $X$ et $Y$. Problème : auncun dataset existant possédant des paires d'audio et de BDs. On va donc chercher à entraîner notre GAN pour qu'il puisse associer des échantillons $x\\in \\{x_i\\}$ à des échantillons $y\\in\\{y_i\\}, de manière non supervisée, en détectant lui-même les caractéristiques pour associer une paire. On s'inspire de l'article *Unpaired Image-to-Image Translation\n",
    "using Cycle-Consistent Adversarial Networks*, qui introduisent les *Cycle-Consistent GANs*. Ils utilisent ce réseau pour passer d'images à images. Dans notre cas, les espaces $X$ et $Y$ ne sont pas aussi similaires, il va peut être falloir injecter des conditions supplémentaires (structure, rythme, couleur, texte, sens --> CLIP), pour aider notre GAN à générer des paires qui ont du sens.\n",
    "\n",
    "En pratique, on a besoin d'entraîner deux générateurs\n",
    "$$\n",
    "G : X \\longrightarrow Y \\\\\n",
    "F : Y \\longrightarrow X\n",
    "$$\n",
    "\n",
    "Chacun associé à un discrimnateur, dont la loss est celle d'un GAN classique, avec une légère différence : on remplace la *negative log likelihood* par une erreur des moindres carrés. Cette loss s'est prouvée plus stable pour ce type de GAN :\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{LSGAN} (G, D_Y, X, Y) =\\: &\\mathbf{E}_{y\\sim p_{data}(y)}[(D_Y(y) - 1)^2]\\\\ + \\: &\\mathbf{E}_{x\\sim p_{data}(x)}[D_Y(G(x))^2]\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "**Aparté : fondamentalement, je pense que $G(x)$, $D_Y(x)$, ou $F(y)$, etc... s'écrivent $G(z|x)$, $D_Y(z|x)$, ou $F(z|y)$, où $z$ est un bruit gaussien, et x, y sont les vecteurs conditions pour les GANs, qu'on concatène à z.**\n",
    "\n",
    "\n",
    "Cette architecture permet d'ajouter une *cycle-loss*, qui détermine si les données sont bien reconstruites lors de l'application successive de $G$ et $F$ :\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{cyc} =\\: &\\mathbb{E}_{x\\sim p_{data}(x)}[||F(G(x)) - x||]\\\\ + \\:&\\mathbb{E}_{y\\sim p_{data}(y)}[||G(F(y)) - y||]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Pour l'architecture, l'article s'inspire de Johnson et al. *Perceptual losses\n",
    "for real-time style transfer and super-resolution* : \n",
    "1. Générateurs (structure encodeur-décodeur):\n",
    " - deux stride-2 convolutions\n",
    " - quelques blocs résiduels\n",
    " - deux stride-$\\frac{1}{2}$ convolutions\n",
    " $\\\\$\n",
    " $\\longrightarrow$ donc pour ces GANs là, on n'augmente pas la dimension comme vu dans les cGANs (on peut quand même modifier si jamais nos espaces $X$ et $Y$ n'ont pas les mêmes dimensions). Nos espaces latents auront peut être des dimensions différentes (par exemple : peut être besoin de plus grandes dimensions dans l'espace latent audio, si on utilise plein de conditions supplémentaires dans l'espace BD). \n",
    "\n",
    " 2. Discriminateurs :\n",
    " - 70x70 Patch-GANs : faits pour discriminer des grosses images (1080p par exemple). Ils ne discriminent que sur des portions de 70 par 70. Peut être pas nécessaire pour nous si on travaille sur des espaces latents de faibles dimensions. Mais peux justement permettre de travailler sur des espaces latents plus complexes sans trop augmenter la charge de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834b2ad",
   "metadata": {},
   "source": [
    "### Générateur $G$ : $X$(BDs) $\\longrightarrow$ $Y$(audios)\n",
    "\n",
    "$G$ part de $X$ l'espace latent BDs pour générer $Y$ l'espace latent audio. Comment on utilisera des conditions supplémentaires pour influencer la génération, on peut se permettre de ne garder que peut d'information dans $X$. Ainsi, on choisit de générer $X$ (partie VAE) comme un vecteur 1D plutôt que comme une image. On ne garde ainsi qu'une idée du style et on oublie les informations de structure spatiale qui seront utilisées plus efficacement dans les autres conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0fbf36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_G(nn.Module):\n",
    "    def __init__(self, z_dim, latent_x_dim, x_ch, init_dim):\n",
    "        '''\n",
    "        Inputs :\n",
    "         - z_dim : dimension du vecteur bruit seed du générateur\n",
    "        Dimensions des espaces latents: \n",
    "         - latent_x_dim : dimension de l'espace latent X d'où est tirée la condition du générateur G.\n",
    "            On peut générer l'espace latent BDs comme un vecteur, car on ne veut l'utiliser que pour avoir une \n",
    "            notion du style\n",
    "         - pas besoin de prendre latent_y_dim pour connaître la dimension de l'objectif si l'architecture\n",
    "         est adaptée en fonction de la dimension de départ init_dim (voir juste en dessous)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim # dimension du vecteur bruit \n",
    "\n",
    "        # dimensions des espaces X et Y (on suppose dim(X) < dim(Y))\n",
    "        # G opère donc une déconvolution pour passer de X à Y\n",
    "        self.x_dim = latent_x_dim # dimension du vecteur condition (espace BDs X)\n",
    "\n",
    "        self.init_dim  = init_dim # dimension (H ou W) de l'image 2D à l'entrée du CNN\n",
    "        # idéalement dim(Y) = dim_init * 2**(n_upsample) pour qu'on puisse facilement arriver à la bonne dimension (ici n_upsample = 2) \n",
    "        self.num_ch = x_ch # nombre de canaux arbitraire (dépend de l'espace latent qu'on choisit, peut être des autres conditions aussi)\n",
    "        \n",
    "        # Première couche linéaire\n",
    "        self.l1 = nn.Sequential( \n",
    "            nn.Linear(self.z_dim + self.x_dim, self.num_ch * self.init_dim * self.init_dim) \n",
    "        )\n",
    "\n",
    "        # Réseau convolutionnel --> déconvolution d'une petite dimension vers une grande dimension\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(self.num_ch),\n",
    "            nn.Upsample(scale_factor=2), # (init_dim)x(init_dim) --> (2*init_dim)x(2*init_dim)\n",
    "            nn.Conv2d(self.num_ch, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2), # (2*init_dim)x(2*init_dim) --> (4*init_dim)x(4*init_dim)\n",
    "            nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, cat_xz):\n",
    "        # Ici, cat est la concaténation du bruit z et du vecteur x\n",
    "        output1 = self.l1(cat_xz) \n",
    "        # reshape en image\n",
    "        output2 = output1.view(output1.shape[0], self.num_ch, self.init_dim, self.init_dim) \n",
    "        # CNN\n",
    "        output = self.model(output2) \n",
    "        # output correspond normalement à une approximation d'un échantillon de l'espace Y\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a74592dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100 # arbitraire\n",
    "x_dim = 100 # dimension d'un vecteur\n",
    "x_ch = 32\n",
    "y_dim = 64 # arbitraire, tenseur 2D 64x64\n",
    "init_dim = 64 // (2**2)\n",
    "G = Generator_G(z_dim, x_dim, x_ch, init_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d4ea8",
   "metadata": {},
   "source": [
    "### Discriminateur $D_Y$\n",
    "\n",
    "$D_Y$ vérifie que $G(x) = \\hat{y} \\approx y$. Il prend donc également en condition le vecteur $x$, injecté à l'avant dernière couche du CNN. Le discriminateur va donc prendre en entrée des données 2D, on peut donc utiliser une structure patchGAN $N$x$N$, qui permettra de réduire la quantité de calculs de la discrimination, et d'avoir un espace latent $Y$ plus complexe.\n",
    "\n",
    "Un discriminateur de type PatchGAN ne sort pas un scalaire de prédiction mais une grille de parédiction pour chaque patche de données. La couche linéaire à la fin du discriminateur conditionnel vu juste avant doit donc nécessairement disparaître car on veut une sortie 2D pour le discriminateur. Il faut donc trouver un autre moyen pour injecter la condition $X$. Une solution est de créer la taille des données $Y$ lors de passage successif dans les convolutions du CNN. A la couche où on a décidé d'injecter $X$, on utilise la taille de $Y$ pour étendre ou compresser $X$ (normalement étendre). on en fait une matrice de même taille que $Y$ en concaténant plusieurs vecteurs, et pusi on la concatène aux données entre deux convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd47a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator_DY(nn.Module):\n",
    "    def __init__(self, y_ch, latent_x_dim, ndf=64):\n",
    "        \"\"\"\n",
    "        num_ch_y     : Canaux de l'image Y (ex: 3 pour RGB)\n",
    "        latent_x_dim : Dimension du vecteur condition X (ex: 64, 128...)\n",
    "        ndf          : Nombre de filtres de base (ex: 64)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Partie avant la concaténation (sans la condition)\n",
    "        self.block_y = nn.Sequential(\n",
    "            nn.Conv2d(y_ch, ndf, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Dimension de la convolution après la concaténation\n",
    "        input_dim_fused = ndf + latent_x_dim\n",
    "        \n",
    "        # CNN post-concaténation (avec la condition)\n",
    "        self.block_fused = nn.Sequential(\n",
    "            nn.Conv2d(input_dim_fused, ndf * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "        # pas de Flatten/Linear à la fin, ou de .view() dans le forward donc la sortie sera en 2D \n",
    "        # --> prédiction pour chaque patch d'une image Y\n",
    "\n",
    "    def forward(self, y_input, x_condition):\n",
    "\n",
    "        # on rentre Y dans la première partie du réseau\n",
    "        feature_map = self.block_y(y_input)\n",
    "\n",
    "        # on en ressort les dimensions\n",
    "        batch_size, _, M_height, M_width = feature_map.size()\n",
    "        \n",
    "        # on reshape la condition X en fonction\n",
    "        x_reshaped = x_condition.view(batch_size, -1, 1, 1)\n",
    "        x_expanded = x_reshaped.expand(batch_size, -1, M_height, M_width)\n",
    "        \n",
    "        # On empile les features de l'image et la condition étendue\n",
    "        combined_input = torch.cat([feature_map, x_expanded], dim=1)\n",
    "        \n",
    "        # partie finale (CNN)\n",
    "        output = self.block_fused(combined_input)\n",
    "        \n",
    "        # l'output est donc  de dimension \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3cc6cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 100 # dimension d'un vecteur\n",
    "num_ch_y = 3 # arbitraire\n",
    "DY = Discriminator_DY(num_ch_y, x_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e9253",
   "metadata": {},
   "source": [
    "### Générateur $F$ : $Y$(audios) $\\longrightarrow$ $X$(BDs)\n",
    "\n",
    "Contrairement à $G$, qui développait les données, $F$ a pour but de compresser l'espace $Y$ vers $X$. On veut que les deux générateurs forment un équilibre lors de l'apprentissage. On a également les même dimensions que dans $G$, mais inversées. Il es dont logique que l'architecture de $F$ soit symétrique à celle de $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef4d7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_F(nn.Module):\n",
    "    def __init__(self, z_dim, latent_x_dim, x_ch, y_ch, init_dim):\n",
    "        '''\n",
    "        Inputs :\n",
    "         - latent_x_dim : dimension du vecteur de sortie (espace X)\n",
    "         - latent_y_dim : nombre de canaux de l'espace latent d'entrée Y\n",
    "         - z_dim        : dimension du bruit gaussien ajouté\n",
    "         - init_dim     : dimension spatiale de référence\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.x_dim = latent_x_dim\n",
    "        self.y_ch = y_ch\n",
    "        self.z_dim = z_dim     \n",
    "        self.init_dim = init_dim\n",
    "        self.internal_ch = x_ch\n",
    "\n",
    "        # Le réseau prend maintenant en entrée canaux de Y + canaux de Z\n",
    "        input_channels = self.y_ch + self.z_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(input_channels, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # equivalent à un Downsample\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2), \n",
    "\n",
    "            nn.Conv2d(256, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, self.internal_ch, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(self.internal_ch),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.flat_dim = self.internal_ch * self.init_dim * self.init_dim\n",
    "        \n",
    "        self.final_head = nn.Sequential(\n",
    "            nn.Linear(self.flat_dim, self.x_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_y, z):\n",
    "\n",
    "        # comme on ajoute z à une matrice 2D, il faut l'étendre (comme la condition X dans DY)\n",
    "        batch_size, _, h, w = input_y.size()\n",
    "        z_expanded = z.view(batch_size, -1, 1, 1).expand(batch_size, -1, h, w)\n",
    "        combined_input = torch.cat([input_y, z_expanded], dim=1)\n",
    "        \n",
    "        features = self.model(combined_input)\n",
    "        features_flat = features.view(features.shape[0], -1)\n",
    "        output_x = self.final_head(features_flat)\n",
    "        \n",
    "        return output_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8fd539a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100 # arbitraire\n",
    "x_dim = 100 # dimension d'un vecteur\n",
    "x_ch = 32\n",
    "y_ch = 64 # arbitraire, tenseur 2D 64x64\n",
    "init_dim = 64 // (2**2)\n",
    "F = Generator_F(z_dim, x_dim, x_ch, x_ch, init_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f08ce",
   "metadata": {},
   "source": [
    "### Discriminateur $D_X$\n",
    "\n",
    "Ce discriminateur agit sur l'espace $X$. Les données sont donc 1D, et plus simples que pour $D_Y$, on n'a pas besoin d'utiliser un discriminateur de type PatchGAN. On utilise un discriminateur conditionnée classique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_DX(nn.Module):\n",
    "    def __init__(self, x_ch, latent_x_dim, cond_dim, ndf=64):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        num_ch_y     : Canaux de l'image Y (ex: 3 pour RGB)\n",
    "        latent_x_dim : Dimension du vecteur condition X (ex: 64, 128...)\n",
    "        ndf          : Nombre de filtres de base (ex: 64)\n",
    "        \"\"\"\n",
    "        self.latent_x_dim = latent_x_dim\n",
    "        self.cond_dim = cond_dim # taille de la condition\n",
    "        self.x_ch = x_ch # nombre de canaux, dépend de l'espace latent audio\n",
    "\n",
    "        self.stride_size = 2 # valeur du stride pour retrouver la dimension lors du reshape\n",
    "\n",
    "        #  modèle de convolution classique, on cherche à densifier l'information des images à chaque couche\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(num_ch, 64, kernel_size=4, stride=2, padding=1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 512, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        # A cette étape, il faut aplatir les données et y concaténer la condition y\n",
    "        # calcul de la dimension du reshape :\n",
    "        self.image_dim = int(latent_x_dim / (self.stride_size ** 3)) # on a appliqué 3 strides de 2\n",
    "        self.flat_dim = 512 * self.image_dim * self.image_dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flat_dim + self.cond_dim, 1), # ce n'est plus une convolution car vecteur plat\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # on rentre x et y car on commence par traiter seulement x avant d'injecter y\n",
    "        features = self.model(x)\n",
    "        \n",
    "        # Aplatissement vers la flat dimension\n",
    "        features_flat = features.view(features.size(0), -1) \n",
    "        \n",
    "        # Concaténation avec le vecteur condition y\n",
    "        combined = torch.cat([features_flat, y], dim=1)\n",
    "        \n",
    "        # Prédiction finale\n",
    "        output = self.classifier(combined)\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
