{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09041d81",
   "metadata": {},
   "source": [
    "# VQ-VAE Hiérarchique pour Audio\n",
    "## Inspiré de \"I Hear Your True Colors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0638842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcab0f2",
   "metadata": {},
   "source": [
    "# Paramètres généraux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d11edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64      # dimension de chaque latent vector\n",
    "num_embeddings = 512 # taille du codebook\n",
    "num_levels = 2       # nombre de niveaux hiérarchiques\n",
    "batch_size = 16\n",
    "lr = 2e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe262b",
   "metadata": {},
   "source": [
    "# Dataset exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567eca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyAudioDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, seq_len=16384):\n",
    "        self.data = torch.randn(num_samples, 1, seq_len) # audio mono\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = DummyAudioDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0d540",
   "metadata": {},
   "source": [
    "# Vector Quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da5e6cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # z : (B, C, T)\n",
    "        z_flattened = z.permute(0,2,1).contiguous().view(-1, self.embedding_dim) # (B*T, C)\n",
    "        distances = (\n",
    "            torch.sum(z_flattened**2, dim=1, keepdim=True)\n",
    "            + torch.sum(self.embedding.weight**2, dim=1)\n",
    "            - 2 * torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "        )\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        z_q = self.embedding(encoding_indices).view(z.size(0), z.size(2), z.size(1)).permute(0,2,1)\n",
    "        \n",
    "        # loss\n",
    "        e_latent_loss = F.mse_loss(z_q.detach(), z)\n",
    "        q_latent_loss = F.mse_loss(z_q, z.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        z_q = z + (z_q - z).detach()  # straight-through estimator\n",
    "        return z_q, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298f1586",
   "metadata": {},
   "source": [
    "# Encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "900d91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_dim=128, latent_dim=latent_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, hidden_dim, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, 4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(hidden_dim, latent_dim, 3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        z = self.conv3(x)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cb78d",
   "metadata": {},
   "source": [
    "# Décodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f9c5568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=latent_dim, hidden_dim=128, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.ConvTranspose1d(latent_dim, hidden_dim, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose1d(hidden_dim, hidden_dim, 4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(hidden_dim, out_channels, 3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.relu(self.conv1(z))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x_recon = self.conv3(x)\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f44fd7",
   "metadata": {},
   "source": [
    "# VQ-VAE hiérarchique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7976aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalVQVAE(nn.Module):\n",
    "    def __init__(self, num_levels=num_levels):\n",
    "        super().__init__()\n",
    "        self.num_levels = num_levels\n",
    "        self.encoders = nn.ModuleList([AudioEncoder() for _ in range(num_levels)])\n",
    "        self.decoders = nn.ModuleList([AudioDecoder() for _ in range(num_levels)])\n",
    "        self.quantizers = nn.ModuleList([VectorQuantizer(num_embeddings, latent_dim) for _ in range(num_levels)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_list = []\n",
    "        loss_list = []\n",
    "        for i in range(self.num_levels):\n",
    "            z = self.encoders[i](x)\n",
    "            z_q, vq_loss = self.quantizers[i](z)\n",
    "            x = self.decoders[i](z_q)  # reconstruction intermédiaire\n",
    "            z_list.append(z_q)\n",
    "            loss_list.append(vq_loss)\n",
    "        return x, sum(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ba6b0",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df59b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = HierarchicalVQVAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2bc22",
   "metadata": {},
   "source": [
    "# Boucle d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7fc924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.9986\n",
      "Epoch 2/5, Loss: 0.9954\n",
      "Epoch 3/5, Loss: 10.7252\n",
      "Epoch 4/5, Loss: 151.0423\n",
      "Epoch 5/5, Loss: 460.8399\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, loss_vq = model(batch)\n",
    "        recon_loss = F.mse_loss(recon, batch)\n",
    "        loss = recon_loss + loss_vq\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
